<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Eric Slyman</title> <meta name="author" content="Eric Slyman"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ericslyman.com/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/media/">media</a> </li> <li class="nav-item "> <a class="nav-link" href="/studies/">studies</a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/cv_slyman.pdf">cv (pdf)</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Eric</span> Slyman </h1> <p class="desc">ML @ Adobe | AI/CS PhD @ Oregon State University</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/me-480.webp 480w, /assets/img/me-800.webp 800w, /assets/img/me-1400.webp 1400w, " sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"></source> <img src="/assets/img/me.jpeg?274b410709ff5176fb3a970812b660bf" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="me.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="more-info"> </div> </div> <div class="clearfix"> <h1 id="about">About</h1> <p>I’m an ML Engineer/Researcher at <strong>Adobe</strong>, working on model <strong>evaluation</strong>, <strong>safety/harm mitigation</strong>, and <strong>intelligent prompting</strong> for <strong>Firefly</strong>. I helped launch the newest generation of Firefly’s image generation and editing models, with a focus on <strong>reliable behavior</strong> and <strong>user-intent following</strong> in creative workflows. I work on <a href="https://www.cc.gatech.edu/people/oliver-brdiczka" rel="external nofollow noopener" target="_blank">Oliver Brdiczka</a>’s team in Adobe’s Applied Science &amp; Machine Learning org, work closely with the Creative Intelligence Lab in Adobe Research, and collaborate with Legal and Applied Ethics teams to build end-to-end, responsible solutions.</p> <p>Previously, I completed my PhD at the intersection of multimodal AI, human–computer interaction, and fairness in the <a href="https://eecs.oregonstate.edu/ai-degree-program" rel="external nofollow noopener" target="_blank">Artificial Intelligence</a> and <a href="https://eecs.oregonstate.edu/academics/graduate/cs" rel="external nofollow noopener" target="_blank">Computer Science</a> programs at Oregon State University, advised by <a href="https://web.engr.oregonstate.edu/~leestef/" rel="external nofollow noopener" target="_blank">Stefan Lee</a> (and <a href="https://minsuk.com/" rel="external nofollow noopener" target="_blank">Minsuk Kahng</a> prior to his move to Google).</p> <p>My work evaluates large-scale vision–language models (e.g., CLIP, ViLBERT, LLaVA, diffusion-based TTI), audits the real-world steps needed to ship them responsibly, and designs mitigations that promote <strong>reliable, human-centered</strong> outcomes across the model lifecycle—from data and training to deployment and monitoring.</p> <p>Beyond research and engineering, I co-led OSU’s <a href="https://www.aigsa.club/" rel="external nofollow noopener" target="_blank">AI Graduate Student Association</a> and helped start the <a href="https://www.aigsa.club/aiasp/" rel="external nofollow noopener" target="_blank">AI Application Support Program</a> to mentor applicants—especially those from underrepresented backgrounds.</p> <p><strong>Current focus:</strong> Responsible generative AI; evaluation at scale; reliability &amp; user-intent alignment; dataset/process transparency; practical tooling that meets industry constraints.</p> <hr> <h3 id="public-speaking--media">public speaking &amp; media</h3> <p>I’m passionate about public speaking and outreach (including invited talks at places like <strong>Apple</strong>, <strong>Google</strong>, and <strong>Sony</strong>). A few highlights include:</p> <ul> <li> <em>“Oregon and Washington graduate students tackle problem of bias in AI”</em> — <a href="https://www.opb.org/article/2024/07/19/think-out-loud-oregon-washington-graduate-students-tackle-problem-bias-ai/" rel="external nofollow noopener" target="_blank">OPB, Think Out Loud</a> </li> <li> <em>“OSU researcher works to screen the bias out of AI”</em> — <a href="https://www.ijpr.org/show/the-jefferson-exchange/2024-07-12/mon-9-am-osu-researcher-works-to-screen-the-bias-out-of-ai" rel="external nofollow noopener" target="_blank">Jefferson Public Radio (NPR)</a> </li> <li> <em>“Adobe Researchers Develop New Training Technique to Make AI Less Socially Biased”</em> — <a href="https://www.techtimes.com/articles/306065/20240626/adobe-researchers-develop-new-training-technique-make-ai-less-socially.htm" rel="external nofollow noopener" target="_blank">TechTimes</a> </li> </ul> <p>For more, see the <a href="/media">media page</a> — and feel free to <strong>reach out for speaking engagements</strong>.</p> </div> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/mmb-480.webp 480w, /assets/img/publication_preview/mmb-800.webp 800w, /assets/img/publication_preview/mmb-1400.webp 1400w, " sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/mmb.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mmb.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="slyman2025judge" class="col-sm-8"> <div class="title">Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles</div> <div class="author"> <em>Eric Slyman</em>, <a href="https://mehrab-tanjim.github.io/" rel="external nofollow noopener" target="_blank">Mehrab Tanjim</a>, <a href="https://kushalkafle.com/" rel="external nofollow noopener" target="_blank">Kushal Kafle</a>, and <a href="https://web.engr.oregonstate.edu/~leestef/" rel="external nofollow noopener" target="_blank">Stefan Lee</a> </div> <div class="periodical"> <em>International Conference on Computer Vision</em>, Oct 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2509.08777" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/mmb_poster.png" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/mmb-judge/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>Multimodal large language models (MLLMs) are increasingly used to evaluate text-to-image (TTI) generation systems, providing automated judgments based on visual and textual context. However, these "judge" models often suffer from biases, overconfidence, and inconsistent performance across diverse image domains. While prompt ensembling has shown promise for mitigating these issues in unimodal, text-only settings, our experiments reveal that standard ensembling methods fail to generalize effectively for TTI tasks. To address these limitations, we propose a new multimodal-aware method called Multimodal Mixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt ensemble approach augmented by image clustering, allowing the judge to dynamically assign prompt weights based on the visual characteristics of each sample. We show that MMB improves accuracy in pairwise preference judgments and greatly enhances calibration, making it easier to gauge the judge’s true uncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB outperforms existing baselines in alignment with human annotations and calibration across varied image content. Our findings highlight the importance of multimodal-specific strategies for judge calibration and suggest a promising path forward for reliable large-scale TTI evaluation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">slyman2025judge</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Slyman, Eric and Tanjim, Mehrab and Kafle, Kushal and Lee, Stefan}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Computer Vision}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/fairdedup-480.webp 480w, /assets/img/publication_preview/fairdedup-800.webp 800w, /assets/img/publication_preview/fairdedup-1400.webp 1400w, " sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/fairdedup.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fairdedup.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="slyman2024fairdedup" class="col-sm-8"> <div class="title">FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication</div> <div class="author"> <em>Eric Slyman</em>, <a href="https://web.engr.oregonstate.edu/~leestef/" rel="external nofollow noopener" target="_blank">Stefan Lee</a>, <a href="https://research.adobe.com/person/scott-cohen/" rel="external nofollow noopener" target="_blank">Scott Cohen</a>, and <a href="https://kushalkafle.com/" rel="external nofollow noopener" target="_blank">Kushal Kafle</a> </div> <div class="periodical"> <em>Computer Vision and Patern Recognition</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2404.16123" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Video</a> <a href="/assets/pdf/fairdedup_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/fairdedup/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>Recent dataset deduplication techniques have demonstrated that content-aware dataset pruning can dramatically reduce the cost of training Vision-Language Pretrained (VLP) models without significant performance losses compared to training on the original dataset. These results have been based on pruning commonly used image-caption datasets collected from the web – datasets that are known to harbor harmful social biases that may then be codified in trained models. In this work, we evaluate how deduplication affects the prevalence of these biases in the resulting trained models and introduce an easy-to-implement modification to the recent SemDeDup algorithm that can reduce the negative effects that we observe. When examining CLIP-style models trained on deduplicated variants of LAION-400M, we find our proposed FairDeDup algorithm consistently leads to improved fairness metrics over SemDeDup on the FairFace and FACET datasets while maintaining zero-shot performance on CLIP benchmarks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">slyman2024fairdedup</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Slyman, Eric and Lee, Stefan and Cohen, Scott and Kafle, Kushal}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Vision and Patern Recognition}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <div class="abstract hidden"> <div style="text-align: center;"> <figure> <iframe src="https://www.youtube.com/watch?v=6EfEM-hgIn8" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen width="auto" height="auto"></iframe> </figure> </div> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/publication_preview/vlslice-480.webp 480w, /assets/img/publication_preview/vlslice-800.webp 800w, /assets/img/publication_preview/vlslice-1400.webp 1400w, " sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/vlslice.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vlslice.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="slyman2023vlslice" class="col-sm-8"> <div class="title">VLSlice: Interactive Vision-and-Language Slice Discovery</div> <div class="author"> <em>Eric Slyman</em>, <a href="https://minsuk.com/" rel="external nofollow noopener" target="_blank">Minsuk Kahng</a>, and <a href="https://web.engr.oregonstate.edu/~leestef/" rel="external nofollow noopener" target="_blank">Stefan Lee</a> </div> <div class="periodical"> <em>International Conference on Computer Vision</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2309.06703" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Video</a> <a href="https://github.com/slymane/vlslice" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/vlslice_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/vlslice/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>Recent work in vision-and-language demonstrates that large-scale pretraining can learn generalizable models that are efficiently transferable to downstream tasks. While this may improve dataset-scale aggregate metrics, analyzing performance around hand-crafted subgroups targeting specific bias dimensions reveals systemic undesirable behaviors. However, this subgroup analysis is frequently stalled by annotation efforts, which require extensive time and resources to collect the necessary data. Prior art attempts to automatically discover subgroups to circumvent these constraints but typically leverages model behavior on existing task-specific annotations and rapidly degrades on more complex inputs beyond "tabular" data, none of which study vision-and-language models. This paper presents VLSlice, an interactive system enabling user-guided discovery of coherent representation-level subgroups with consistent visiolinguistic behavior, denoted as vision-and-language slices, from unlabeled image sets. We show that VLSlice enables users to quickly generate diverse high-coherency slices in a user study (n=22) and release the tool publicly.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">slyman2023vlslice</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Slyman, Eric and Kahng, Minsuk and Lee, Stefan}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VLSlice: Interactive Vision-and-Language Slice Discovery}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Computer Vision}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{15291-15301}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <div class="abstract hidden"> <div style="text-align: center;"> <figure> <iframe src="https://drive.google.com/file/d/1mOuvjphNb2xNDC7shoGbPwyjbfArwud4/preview" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen width="auto" height="auto"></iframe> </figure> </div> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%73%6C%79%6D%61%6E%65[%61%74]%6F%72%65%67%6F%6E%73%74%61%74%65[%64%6F%74]%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-2481-7942" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=HruWYeIAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/slymane" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/ericslyman" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/EricSlyman" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <div class="contact-note"> Reach out via LinkedIn for contact info. </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Eric Slyman. Last updated: October 18, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>